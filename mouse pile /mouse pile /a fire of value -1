import numpy as np

# Define the matrix dimensions
ROWS = 3
COLS = 4

# Define the rewards and fire locations
rewards = np.zeros((ROWS, COLS))
rewards[0, 3] = 1  # Maximum reward at (1, 4)
rewards[1, 3] = 1  # Maximum reward at (2, 4)
rewards[0, 0] = -1  # Fire at (1, 1)
rewards[1, 0] = -1  # Fire at (2, 1)

# Define the initial Q-values
Q = np.zeros((ROWS, COLS, 4))  # 4 actions: up, down, left, right

# Define parameters for Q-learning
gamma = 0.8  # Discount factor
alpha = 0.9  # Learning rate
epsilon = 0.1  # Exploration rate

# Define actions
actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']

# Define function to choose action using epsilon-greedy policy
def choose_action(state):
    if np.random.uniform(0, 1) < epsilon:
        return np.random.choice(4)  # Explore
    else:
        return np.argmax(Q[state[0], state[1]])

# Define function to take action and get next state and reward
def take_action(state, action):
    if action == 0:  # Up
        next_state = (max(state[0] - 1, 0), state[1])
    elif action == 1:  # Down
        next_state = (min(state[0] + 1, ROWS - 1), state[1])
    elif action == 2:  # Left
        next_state = (state[0], max(state[1] - 1, 0))
    else:  # Right
        next_state = (state[0], min(state[1] + 1, COLS - 1))
    reward = rewards[next_state[0], next_state[1]]
    return next_state, reward

# Q-learning algorithm
def q_learning():
    for _ in range(500):  # Run for 500 episodes
        state = (0, 0)  # Initial state
        while state != (0, 3) and state != (1, 3):  # Until reaching maximum reward states
            action = choose_action(state)
            next_state, reward = take_action(state, action)
            best_next_action = np.argmax(Q[next_state[0], next_state[1]])
            Q[state[0], state[1], action] += alpha * (reward + gamma * Q[next_state[0], next_state[1], best_next_action] - Q[state[0], state[1], action])
            state = next_state

# Run Q-learning
q_learning()

# Print the learned Q-values
print("Learned Q-values:")
for i in range(ROWS):
    for j in range(COLS):
        print(f"State ({i+1},{j+1}): {dict(zip(actions, Q[i, j]))}")

# Function to find the optimal path
def find_optimal_path():
    state = (0, 0)  # Start from initial state
    optimal_path = [(1, 1)]  # Store optimal path
    while state != (0, 3) and state != (1, 3):  # Until reaching maximum reward states
        action = np.argmax(Q[state[0], state[1]])
        next_state, _ = take_action(state, action)
        optimal_path.append((next_state[0]+1, next_state[1]+1))  # Adding 1 to convert 0-indexed to 1-indexed
        state = next_state
    return optimal_path

# Find and print the optimal path
print("\nOptimal path:")
optimal_path = find_optimal_path()
print(optimal_path)
