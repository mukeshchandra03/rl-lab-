import numpy as np

# Define the Environment
class Environment:
    def __init__(self):
        self.state = 0

    def step(self, action):
        if action == 0:  # Move left
            self.state -= 1
        elif action == 1:  # Move right
            self.state += 1

        # Define boundaries
        self.state = np.clip(self.state, 0, 4)

        # Define rewards
        if self.state == 4:
            reward = 1
            done = True
        else:
            reward = 0
            done = False

        return self.state, reward, done

# Define the Agent
class Agent:
    def __init__(self):
        self.q_values = np.zeros((5, 2))  # Q-values for each state-action pair

    def choose_action(self, state, epsilon=0.1):
        if np.random.rand() < epsilon:  # Exploration
            return np.random.choice([0, 1])
        else:  # Exploitation
            return np.argmax(self.q_values[state])

    def update_q_values(self, state, action, reward, next_state, alpha=0.1, gamma=0.99):
        next_action = self.choose_action(next_state, epsilon=0)  # Greedy action selection
        td_target = reward + gamma * self.q_values[next_state, next_action]
        td_error = td_target - self.q_values[state, action]
        self.q_values[state, action] += alpha * td_error

# Training
env = Environment()
agent = Agent()

num_episodes = 1000
total_reward = 0

for episode in range(num_episodes):
    state = env.state
    episode_reward = 0
    done = False

    while not done:
        action = agent.choose_action(state)
        next_state, reward, done = env.step(action)
        agent.update_q_values(state, action, reward, next_state)
        state = next_state
        episode_reward += reward

    total_reward += episode_reward

    if (episode + 1) % 100 == 0:
        print(f"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}")
        total_reward = 0

# Testing
state = env.state
done = False

while not done:
    action = agent.choose_action(state, epsilon=0)  # Choose greedy action
    next_state, reward, done = env.step(action)
    state = next_state

print(f"Final State: {state}")
