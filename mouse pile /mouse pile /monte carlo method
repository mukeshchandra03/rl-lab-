import numpy as np

# Define the grid world environment
class GridWorld:
    def __init__(self, width, height, start, goal):
        self.width = width
        self.height = height
        self.start = start
        self.goal = goal
        self.current_state = start

    def reset(self):
        self.current_state = self.start

    def step(self, action):
        x, y = self.current_state
        if action == 'up':
            new_state = (x, max(0, y - 1))
        elif action == 'down':
            new_state = (x, min(self.height - 1, y + 1))
        elif action == 'left':
            new_state = (max(0, x - 1), y)
        elif action == 'right':
            new_state = (min(self.width - 1, x + 1), y)
        else:
            raise ValueError("Invalid action")

        self.current_state = new_state
        if new_state == self.goal:
            reward = 1
            done = True
        else:
            reward = 0
            done = False
        return new_state, reward, done

# Define the agent
class Agent:
    def __init__(self, actions):
        self.actions = actions
        self.Q = {}

    def choose_action(self, state, epsilon):
        if np.random.uniform(0, 1) < epsilon:
            return np.random.choice(self.actions)
        else:
            if state not in self.Q:
                return np.random.choice(self.actions)
            else:
                return max(self.Q[state], key=self.Q[state].get)

    def update_Q(self, episode):
        G = 0
        for s, a, r in reversed(episode):
            G = r + G
            if (s, a) not in self.Q:
                self.Q[(s, a)] = G
            else:
                self.Q[(s, a)] = (self.Q[(s, a)] + G) / 2

# Define the Monte Carlo control algorithm
def monte_carlo_control(env, agent, num_episodes, epsilon):
    for _ in range(num_episodes):
        episode = []
        env.reset()
        state = env.current_state
        done = False
        while not done:
            action = agent.choose_action(state, epsilon)
            next_state, reward, done = env.step(action)
            episode.append((state, action, reward))
            state = next_state
        agent.update_Q(episode)

# Define parameters
width = 5
height = 5
start = (0, 0)
goal = (4, 4)
actions = ['up', 'down', 'left', 'right']
num_episodes = 1000
epsilon = 0.1

# Initialize environment and agent
env = GridWorld(width, height, start, goal)
agent = Agent(actions)

# Apply Monte Carlo control algorithm
monte_carlo_control(env, agent, num_episodes, epsilon)

# Print the learned Q-values
print("Learned Q-values:")
for state_action, value in agent.Q.items():
    print(state_action, value)
