import numpy as np

# Define the grid environment
grid_width = 4
grid_height = 6
num_states = grid_width * grid_height

# Define rewards
rewards = np.zeros((grid_height, grid_width))
rewards[0, 3] = 1  # Maximum reward location
rewards[1, 3] = -1  # Fire location

# Define initial position of the agent
initial_position = (0, 0)

# Define actions: up, down, left, right
actions = ['up', 'down', 'left', 'right']
num_actions = len(actions)

# Define parameters for Q-learning
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.1  # Epsilon for epsilon-greedy strategy

# Initialize Q-table
Q = np.zeros((grid_height, grid_width, num_actions))

# Function to choose an action using epsilon-greedy strategy with exploitation
def choose_action(state):
    exploit_prob = 1 - epsilon
    if np.random.uniform(0, 1) < exploit_prob:
        return actions[np.argmax(Q[state[0], state[1]])]
    else:
        return np.random.choice(actions)

# Function to update Q-values based on Q-learning algorithm
def update_Q(state, action, next_state, reward):
    max_next_Q = np.max(Q[next_state[0], next_state[1]])
    Q[state[0], state[1], actions.index(action)] += alpha * (reward + gamma * max_next_Q - Q[state[0], state[1], actions.index(action)])

# Main Q-learning loop
num_episodes = 1000
for _ in range(num_episodes):
    state = initial_position
    done = False
    while not done:
        action = choose_action(state)
        next_state = (state[0], state[1])
        if action == 'up':
            next_state = (max(0, state[0] - 1), state[1])
        elif action == 'down':
            next_state = (min(grid_height - 1, state[0] + 1), state[1])
        elif action == 'left':
            next_state = (state[0], max(0, state[1] - 1))
        elif action == 'right':
            next_state = (state[0], min(grid_width - 1, state[1] + 1))

        reward = rewards[next_state[0], next_state[1]]
        update_Q(state, action, next_state, reward)

        if next_state == (0, 3) or next_state == (1, 3):
            done = True
        else:
            state = next_state

# Find the optimal policy
optimal_policy = np.zeros((grid_height, grid_width), dtype=str)
for i in range(grid_height):
    for j in range(grid_width):
        optimal_policy[i, j] = actions[np.argmax(Q[i, j])]

print("Optimal Policy:")
print(optimal_policy)

# Print the Q-values for each state-action pair
print("\nQ-values:")
print(Q)
