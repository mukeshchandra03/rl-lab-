import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
import gym

# Define DQN model
class DQN(models.Model):
    def __init__(self, num_actions):
        super(DQN, self).__init__()
        self.dense1 = layers.Dense(64, activation='relu')
        self.dense2 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(num_actions)

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        return self.output_layer(x)

# Define DDQN model
class DDQN(models.Model):
    def __init__(self, num_actions):
        super(DDQN, self).__init__()
        self.dense1 = layers.Dense(64, activation='relu')
        self.dense2 = layers.Dense(64, activation='relu')
        self.advantage = layers.Dense(num_actions)
        self.value = layers.Dense(1)

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        advantage = self.advantage(x)
        value = self.value(x)
        q_values = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))
        return q_values

# Define replay buffer
class ReplayBuffer:
    def __init__(self, buffer_size):
        self.buffer_size = buffer_size
        self.buffer = []

    def add(self, experience):
        self.buffer.append(experience)
        if len(self.buffer) > self.buffer_size:
            self.buffer.pop(0)

    def sample(self, batch_size):
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        return [self.buffer[i] for i in indices]

# Define agent
class Agent:
    def __init__(self, num_actions, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):
        self.num_actions = num_actions
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.gamma = gamma
        self.replay_buffer = ReplayBuffer(buffer_size=10000)
        self.optimizer = optimizers.Adam(learning_rate)
        self.model = DQN(num_actions)
        self.model.compile(optimizer=self.optimizer, loss='mse')  # Compile the model
        # Uncomment below to use DDQN instead
        # self.model = DDQN(num_actions)
        # self.model.compile(optimizer=self.optimizer, loss='mse')  # Compile the model

    def select_action(self, state):
        if np.random.rand() <= self.epsilon:
            return np.random.choice(self.num_actions)
        else:
            q_values = self.model.predict(np.array([state]))
            return np.argmax(q_values[0])

    def train(self, batch_size):
        if len(self.replay_buffer.buffer) < batch_size:
            return
        batch = self.replay_buffer.sample(batch_size)
        states = np.array([experience[0] for experience in batch])
        actions = np.array([experience[1] for experience in batch])
        rewards = np.array([experience[2] for experience in batch])
        next_states = np.array([experience[3] for experience in batch])
        dones = np.array([experience[4] for experience in batch])

        future_rewards = np.max(self.model.predict(next_states), axis=1)
        target_rewards = rewards + self.gamma * future_rewards * (1 - dones)

        target_q_values = self.model.predict(states)
        target_q_values[np.arange(batch_size), actions] = target_rewards

        self.model.train_on_batch(states, target_q_values)

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Initialize environment and agent
env = gym.make('CartPole-v1')
num_actions = env.action_space.n
agent = Agent(num_actions)

# Training loop
num_episodes = 10
batch_size = 32
for episode in range(num_episodes):
    state = env.reset()
    total_reward = 0
    done = False
    while not done:
        action = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)
        agent.replay_buffer.add((state, action, reward, next_state, done))
        agent.train(batch_size)
        total_reward += reward
        state = next_state
    print(f"Episode: {episode + 1}, Total Reward: {total_reward}")

# Evaluate the trained agent
total_rewards = []
num_eval_episodes = 10
for _ in range(num_eval_episodes):
    state = env.reset()
    total_reward = 0
    done = False
    while not done:
        action = np.argmax(agent.model.predict(np.array([state])))
        next_state, reward, done, _ = env.step(action)
        total_reward += reward
        state = next_state
    total_rewards.append(total_reward)

print("Average Total Reward:", np.mean(total_rewards))
